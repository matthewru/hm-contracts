{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading & Preprocessing:**\n",
    "\n",
    "This section of the code loads all of the LaTeX documents we have in our datasets. It then preprocesses the data by splitting them into chunks based on a given chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs loaded: 27\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "tex_files = glob.glob('datasets/*.tex')  # load all .tex files from dataset directory\n",
    "documents = []\n",
    "CHUNK_SIZE = 50\n",
    "\n",
    "def clean_tex_file(text):\n",
    "    # Remove comments (lines starting with %)\n",
    "    text = re.sub(r'(?m)^%.*\\n?', '', text)\n",
    "    # Remove LaTeX commands (basic removal; adjust if needed)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+(\\[[^\\]]*\\])?(\\{[^\\}]*\\})?', '', text)\n",
    "    # Optionally remove math environments (if not needed)\n",
    "    text = re.sub(r'\\$[^\\$]*\\$', '', text)\n",
    "    # Remove measurement units like \"cm\", \"pt\", \"em\" that follow a number\n",
    "    text = re.sub(r'\\b\\d+(,\\d+)?\\s*(cm|pt|em)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    # Delete all remaining symbols: keep only letters, numbers, and whitespace.\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "for filename in tex_files:\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        raw_text = file.read()\n",
    "        cleaned_text = clean_tex_file(raw_text)\n",
    "        documents.append(cleaned_text)\n",
    "\n",
    "def split_into_chunks(text):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+CHUNK_SIZE]) for i in range(0, len(words), CHUNK_SIZE)]\n",
    "\n",
    "document_chunks = [split_into_chunks(doc) for doc in documents]  # create chunks for each document\n",
    "\n",
    "print(\"Number of docs loaded:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Annotation:**\n",
    "\n",
    "Use Doccano, an open source annotation tool, to label and annotate our data. This helps us create high-quality training data for our RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'document_chunks.csv' created with 220 rows.\n"
     ]
    }
   ],
   "source": [
    "#create a csv format of doc chunks\n",
    "\n",
    "import csv\n",
    "\n",
    "# Flatten the document_chunks list and assign document/chunk IDs\n",
    "rows = []\n",
    "for doc_idx, chunks in enumerate(document_chunks, start=1):\n",
    "    for chunk_idx, chunk in enumerate(chunks, start=1):\n",
    "        rows.append({\n",
    "            'doc_id': doc_idx,\n",
    "            'chunk_id': chunk_idx,\n",
    "            'text': chunk\n",
    "        })\n",
    "\n",
    "# Write rows to a CSV file\n",
    "csv_file = 'document_chunks.csv'\n",
    "with open(csv_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    fieldnames = ['doc_id', 'chunk_id', 'text']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file '{csv_file}' created with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create embeddings**\n",
    "\n",
    "We create numerical embeddings for each of the chunks using a sentence transformer (all-MiniLm-L6-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'document_chunks_with_embeddings.csv' created with embeddings for 220 rows.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Read your existing CSV file of document chunks\n",
    "input_csv = 'document_chunks.csv'\n",
    "rows = []\n",
    "with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# Process each row to compute embeddings for the text chunk\n",
    "for row in rows:\n",
    "    text = row['text']\n",
    "    # Compute embedding\n",
    "    embedding = model.encode(text)\n",
    "    # Convert the numpy array to a list and then to a JSON string for CSV storage\n",
    "    row['embedding'] = json.dumps(embedding.tolist())\n",
    "\n",
    "# Write the results to a new CSV file including the embeddings column\n",
    "output_csv = 'document_chunks_with_embeddings.csv'\n",
    "with open(output_csv, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    fieldnames = ['doc_id', 'chunk_id', 'text', 'embedding']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file '{output_csv}' created with embeddings for {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings indexed: 220\n",
      "Retrieved Chunks:\n",
      "0 This Consulting Agreement the Agreement is made and entered into as of by and between The Consultant agrees to provide the following services to the Client This Agreement shall commence on and continue until unless terminated earlier as outlined in Section 7 The Client agrees to pay the Consultant \n",
      "\n",
      "Agreement If additional payment is due this shall be payable within thirty days of the Clients written notification to stop work In the event of termination the Client shall also pay any expenses incurred by Consultant The Client shall assume responsibility for all collection of legal fees necessitated by default \n",
      "\n",
      "Purchase Price is due upon signing this Contract The remaining balance shall be paid upon delivery of the Products Payment shall be made via check wire transfer or credit card The Products shall be delivered to Buyer at on or before Title and risk of loss shall pass to Buyer \n",
      "\n",
      "handling fees A deposit of 30 is due upon signing this Contract The remaining balance shall be paid upon delivery of the Products Payment may be made via cash creditdebit card or certified check The Products shall be delivered to Buyer at on or before Title and risk of loss \n",
      "\n",
      "related to this Contract shall be resolved through binding arbitration in This Contract constitutes the entire agreement between the parties and supersedes any prior agreements or understandings Any amendments to this Contract must be in writing and signed by both parties If any provision of this Contract is held to \n",
      "\n",
      "0 This Contract is made as of This Sales Contract Contract is entered into between a small business specializing in highquality mattresses located at with a mailing address at Seller agrees to sell and Buyer agrees to purchase the following products the Products inclusive of applicable taxes delivery charges and \n",
      "\n",
      "responsible for payment of all services completed up to the termination date This Agreement shall be governed by and construed in accordance with the laws of This Agreement constitutes the entire agreement between the parties Any amendments must be in writing and signed by both parties the parties have executed \n",
      "\n",
      "0 This Contract is made as of This Sales Contract Contract is made and entered into between located at residing at Seller agrees to sell and Buyer agrees to purchase the following tires Products eg Allterrain size 26570R17 brand model includes applicable taxes and fees A deposit of 20 is \n",
      "\n",
      "to this Contract shall be resolved through binding arbitration in This Contract constitutes the entire agreement between the parties and supersedes any prior understandings or agreements Any amendments to this Contract must be in writing and signed by both parties If any provision of this Contract is deemed invalid or \n",
      "\n",
      "due upon signing this Contract with the remaining balance due upon delivery of the tires Payments shall be made via cash creditdebit card or certified check The Products shall be delivered to the Buyer at on or before Buyer shall inspect the Products upon delivery and notify Seller within 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load embeddings and text chunks from CSV\n",
    "embeddings = []\n",
    "texts = []\n",
    "with open('document_chunks_with_embeddings.csv', 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        emb = np.array(json.loads(row['embedding']), dtype=np.float32)\n",
    "        embeddings.append(emb)\n",
    "        texts.append(row['text'])\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Build FAISS index (using L2 distance)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(\"Total embeddings indexed:\", index.ntotal)\n",
    "\n",
    "# Function to query the index\n",
    "def search_query(query, k=10):\n",
    "    query_embedding = model.encode(query, convert_to_numpy=True).astype(np.float32)\n",
    "    distances, indices = index.search(np.expand_dims(query_embedding, axis=0), k)\n",
    "    return [texts[idx] for idx in indices[0]]\n",
    "\n",
    "# Test the retrieval system\n",
    "query = \"What are the payment terms for the contract?\"\n",
    "retrieved_chunks = search_query(query)\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
